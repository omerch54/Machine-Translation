{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eTGhY-AHw4"
      },
      "source": [
        "# 🗫 Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBgSRG1p7ZMq"
      },
      "source": [
        "# Setup and Data Preprocessing \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCaEFscw9LwA",
        "outputId": "3d6b03b7-3e95-4bd0-b42c-04a55f169e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18.0) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchtext-0.18.0 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b9CDVm3o7ZMr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import copy\n",
        "import io\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Callable, Union\n",
        "\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext import vocab as torchtext_vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from timeit import default_timer as timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VU9gI0Sd7ZMu"
      },
      "outputs": [],
      "source": [
        "def seed_everything(s: int):\n",
        "  \"\"\"\n",
        "  This function allows us to set the seed for all of our random functions\n",
        "  so that we can get reproducible results.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  s : int\n",
        "      seed to seed all random functions with\n",
        "  \"\"\"\n",
        "  random.seed(s)\n",
        "  torch.manual_seed(s)\n",
        "  torch.cuda.manual_seed_all(s)\n",
        "  np.random.seed(s)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWHB-3sD7ZMu",
        "outputId": "a3b990c2-be09-4e13-8556-5705f944ac40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 637k/637k [00:00<00:00, 63.4MB/s]\n",
            "100%|██████████| 569k/569k [00:00<00:00, 79.6MB/s]\n",
            "100%|██████████| 24.7k/24.7k [00:00<00:00, 36.4MB/s]\n",
            "100%|██████████| 21.6k/21.6k [00:00<00:00, 32.9MB/s]\n",
            "100%|██████████| 22.9k/22.9k [00:00<00:00, 34.9MB/s]\n",
            "100%|██████████| 21.1k/21.1k [00:00<00:00, 22.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# data loaded from Multi30k dataset, see https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
        "url_base = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "train_urls = (\"train.de.gz\", \"train.en.gz\")\n",
        "val_urls = (\"val.de.gz\", \"val.en.gz\")\n",
        "test_urls = (\"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\")\n",
        "\n",
        "# download and extract data\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "# get tokenizers for English and German\n",
        "de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oGLdP4Dbfn0C"
      },
      "outputs": [],
      "source": [
        "def build_vocab(filepath: str, tokenizer: Callable) -> torchtext_vocab.vocab:\n",
        "    \"\"\"\n",
        "    Builds a vocab based on a given file of sentence and a given tokenizer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        filepath from which to get sentences to generate a vocabulary\n",
        "    tokenizer : Callable\n",
        "        function with which to tokenize the sentences\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torchtext.vocab.Vocab\n",
        "        a PyTorch vocab object\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f: # each string here is a German or English sentence\n",
        "            counter.update(tokenizer(string_))\n",
        "    return torchtext.vocab.vocab(counter, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "\n",
        "# build vocabularies for both languages based on the training data\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "\n",
        "# sets the default behavior for OOV words to be <unk>\n",
        "de_vocab.set_default_index(0)\n",
        "en_vocab.set_default_index(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rA2rAmA8fp3g"
      },
      "outputs": [],
      "source": [
        "def data_process(filepaths: list[str]) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
        "  \"\"\"\n",
        "  Builds a dataset of translated sentences from a list of filepaths\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filepaths : list[str]\n",
        "      a list containing the filepath to a German dataset and a filepath to\n",
        "      the corresponding English dataset\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[tuple[torch.Tensor, torch.Tensor]]\n",
        "      a list of tuples of tensors, where each tuple is a pair of translations\n",
        "      (German, English) in the form of tensors where tokenized words are encoded\n",
        "      as vocabulary indices\n",
        "  \"\"\"\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    # raw_de and raw_en are paired sentences in English and German\n",
        "    de_tensor_ = torch.tensor(de_vocab(de_tokenizer(raw_de)), dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor(en_vocab(en_tokenizer(raw_en)), dtype=torch.long)\n",
        "\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data\n",
        "\n",
        "# clean the data by tokenizing sentences and converting them to tensors\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "di-c_e7XiUxV"
      },
      "outputs": [],
      "source": [
        "# sets default values for the rest of the notebook\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # remember this for later!\n",
        "BATCH_SIZE = 32\n",
        "PAD_IDX = de_vocab[\"<pad>\"]\n",
        "BOS_IDX = de_vocab[\"<bos>\"]\n",
        "EOS_IDX = de_vocab[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-834e9RH7ZMv"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as the collate_fn argument when constructing DataLoaders to collate\n",
        "  lists of samples into batches\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_batch : list[tuple[torch.Tensor, torch.Tensor]]\n",
        "      a list of tuples of tensors, where each tuple is a pair of translations\n",
        "      (German, English) in the form of tensors where tokenized words are encoded\n",
        "      as vocabulary indices\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a tuple of batched German sentences and batched English sentences in the\n",
        "      form of tensors of vocabulary indices\n",
        "  \"\"\"\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "# create data loaders for the training, validation, and test data\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jesM-6Xz7ZMv"
      },
      "source": [
        "# Part 1: Building a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ey2vJnHF7ZMx"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    '''Applies a residual connection followed by a layer norm to any sublayer'''\n",
        "    def __init__(self, size: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Initializes a SublayerConnection module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            size of the expected input to the module\n",
        "        dropout : float\n",
        "            dropout value to be used after the sublayer\n",
        "        \"\"\"\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, sublayer: Union[nn.Module, Callable]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the model. Normalizes the input, applies the sublayer,\n",
        "        performs a dropout, and then performs a residual connection.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor\n",
        "        sublayer : nn.Module or Callable\n",
        "            layer (or function that acts as a layer) that a residual connection\n",
        "            is performed over\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' Implements the two-layer feedforward neural network used in the transformer.'''\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes a PositionwiseFeedForward module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d_model : int\n",
        "            size of the input into and output out of the feedforward layer\n",
        "        d_ff : int\n",
        "            hidden size of the feedforward layer\n",
        "        dropout : float\n",
        "            dropout value used at end of feedforward layer\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the model. Normalizes the input, applies the sublayer,\n",
        "        performs a dropout, and then performs a residual connection.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor\n",
        "        sublayer : nn.Module\n",
        "            layer that a residual connection is performed over\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
        "\n",
        "def clones(module: nn.Module, N: int) -> nn.ModuleList:\n",
        "    \"\"\"\n",
        "    Creates a nn.ModuleList of N identical copies of the inputted module\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    module : nn.Module\n",
        "        module to clone\n",
        "    N : int\n",
        "        number of layers to clone\n",
        "\n",
        "    Returns\n",
        "    nn.ModuleList\n",
        "        iterable list of the cloned layers\n",
        "    ----------\n",
        "\n",
        "    \"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtCd2gPf7ZMx"
      },
      "source": [
        "## Building an Encoder Block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RgbIQ-7i7ZMy"
      },
      "outputs": [],
      "source": [
        "class ManualEncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 size: int,\n",
        "                 dropout: float,\n",
        "                 nhead: int,\n",
        "                 dim_ff: int):\n",
        "        \"\"\"\n",
        "        Initializes a ManualEncoderLayer module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            size of the input into and output out of the feedforward layer\n",
        "        dropout : float\n",
        "            dropout value used for attention, residual connection, and\n",
        "            feedforward layers\n",
        "        nhead : int\n",
        "            number of attention heads used in multi-head attention\n",
        "        dim_ff : int\n",
        "            hidden dimension of the feedforward layer\n",
        "        \"\"\"\n",
        "        super(ManualEncoderLayer, self).__init__()\n",
        "        # TODO: Initialize the necessary pieces of the encoder block\n",
        "        # Multi-head attention layer\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=size, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Feed-forward layer\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model=size, d_ff=dim_ff, dropout=dropout)\n",
        "\n",
        "        # Sublayer connections\n",
        "        self.sublayer_attn = SublayerConnection(size, dropout)\n",
        "        self.sublayer_ff = SublayerConnection(size, dropout)\n",
        "\n",
        "        # Size of the layer (used for layer normalization)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward encoding pass for one layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor\n",
        "        padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        x = self.sublayer_attn(x, lambda x: self.self_attn(x, x, x, key_padding_mask=padding_mask)[0])\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        x = self.sublayer_ff(x, self.feed_forward)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgi-mNf7ZMy"
      },
      "source": [
        "### Implementing the Encoder: `ManualEncoder`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "csgSFyW37ZMy"
      },
      "outputs": [],
      "source": [
        "class ManualEncoder(nn.Module):\n",
        "    def __init__(self, layer: nn.Module, N: int):\n",
        "        \"\"\"\n",
        "        Initializes a ManualEncoder module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : nn.Module\n",
        "            a ManualEncoderLayer\n",
        "        N : int\n",
        "            number of encoder layers in the encoder\n",
        "        \"\"\"\n",
        "        super(ManualEncoder, self).__init__()\n",
        "        # TODO: Initialize the necessary pieces of the encoder\n",
        "        # HINT: this mostly consists of making copies of your encoder layers\n",
        "        self.layers = clones(layer, N)\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.norm = nn.LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Full encoder forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor\n",
        "        padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        # Apply final layer normalization\n",
        "        normalized_x = self.norm(x)\n",
        "\n",
        "        return normalized_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PgQTQXr7ZMy"
      },
      "source": [
        "## Building a Decoder Block\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9KQdC8h7ZMz"
      },
      "source": [
        "### Implementing a Decoder Layer: `ManualDecoderLayer`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W3xkpXQk7ZMz"
      },
      "outputs": [],
      "source": [
        "class ManualDecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 size: int,\n",
        "                 dropout: float,\n",
        "                 nhead: int,\n",
        "                 dim_ff: int):\n",
        "        \"\"\"\n",
        "        Initializes a ManualDecoderLayer module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            size of the input into and output out of the feedforward layer\n",
        "        dropout : float\n",
        "            dropout value used for attention, residual connection, and\n",
        "            feedforward layers\n",
        "        nhead : int\n",
        "            number of attention heads used in multi-head attention\n",
        "        dim_ff : int\n",
        "            hidden dimension of the feedforward layer\n",
        "        \"\"\"\n",
        "        super(ManualDecoderLayer, self).__init__()\n",
        "        self.attn_weights = None\n",
        "        # TODO: Initialize the necessary pieces of the decoder block\n",
        "        # Self-attention layer for target sequence\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim=size, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Cross-attention layer for encoder-decoder attention\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim=size, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Feed-forward layer\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model=size, d_ff=dim_ff, dropout=dropout)\n",
        "\n",
        "        # Sublayer connections\n",
        "        self.sublayer_self_attn = SublayerConnection(size, dropout)\n",
        "        self.sublayer_cross_attn = SublayerConnection(size, dropout)\n",
        "        self.sublayer_ff = SublayerConnection(size, dropout)\n",
        "\n",
        "        # Size of the layer (used for layer normalization)\n",
        "        self.size = size\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                src_encoding: torch.Tensor,\n",
        "                src_padding_mask: torch.Tensor,\n",
        "                tgt_mask: torch.Tensor,\n",
        "                tgt_padding_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward decoding pass for one layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor (model's target sequence)\n",
        "        src_encoding : torch.Tensor\n",
        "            output of the encoder\n",
        "        src_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the \"encoded input\" (src_encoding)\n",
        "        tgt_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of tokens in target to\n",
        "            mask attention to\n",
        "        tgt_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input (target sequence)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        x = self.sublayer_self_attn(\n",
        "            x,\n",
        "            lambda x: self.self_attn(\n",
        "                x, x, x,\n",
        "                attn_mask=tgt_mask,\n",
        "                key_padding_mask=tgt_padding_mask\n",
        "            )[0]\n",
        "        )\n",
        "\n",
        "        # Cross-attention sublayer\n",
        "        x = self.sublayer_cross_attn(\n",
        "            x,\n",
        "            lambda x: self.cross_attn(\n",
        "                x, src_encoding, src_encoding,\n",
        "                key_padding_mask=src_padding_mask,\n",
        "                need_weights=True\n",
        "            )[0]\n",
        "        )\n",
        "\n",
        "        # Save attention weights for analysis\n",
        "        _, self.attn_weights = self.cross_attn(\n",
        "            x, src_encoding, src_encoding,\n",
        "            key_padding_mask=src_padding_mask,\n",
        "            need_weights=True\n",
        "        )\n",
        "\n",
        "\n",
        "        return self.sublayer_ff(x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OLow-mp7ZMz"
      },
      "source": [
        "### Implementing the Decoder: `ManualDecoder`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jm9tPCxz7ZMz"
      },
      "outputs": [],
      "source": [
        "class ManualDecoder(nn.Module):\n",
        "    def __init__(self, layer: nn.Module, N: int):\n",
        "        \"\"\"\n",
        "        Initializes a ManualDecoder module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : nn.Module\n",
        "            a ManualDecoderLayer\n",
        "        N : int\n",
        "            number of decoder layers in the decoder\n",
        "        \"\"\"\n",
        "        super(ManualDecoder, self).__init__()\n",
        "        # TODO: Initialize the necessary pieces of the decoder\n",
        "        # HINT: this mostly consists of making copies of your decoder layers\n",
        "        self.layers = clones(layer, N)\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.norm = nn.LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                src_encoding: torch.Tensor,\n",
        "                src_padding_mask: torch.Tensor,\n",
        "                tgt_mask: torch.Tensor,\n",
        "                tgt_padding_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Full decoder forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            input Tensor (model's target sequence)\n",
        "        src_encoding : torch.Tensor\n",
        "            output of the encoder\n",
        "        src_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the \"encoded input\" (src_encoding)\n",
        "        tgt_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of tokens in target to\n",
        "            mask attention to\n",
        "        tgt_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input (target sequence)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        for layer in self.layers:\n",
        "            x = layer(\n",
        "                x, src_encoding,\n",
        "                src_padding_mask=src_padding_mask,\n",
        "                tgt_mask=tgt_mask,\n",
        "                tgt_padding_mask=tgt_padding_mask\n",
        "            )\n",
        "\n",
        "        # Apply final layer normalization\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSFSlHFH7ZMz"
      },
      "source": [
        "## Putting Everything Together\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sx0rGgue8d54"
      },
      "outputs": [],
      "source": [
        "## DO NOT EDIT THIS CELL\n",
        "class TokenEmbedding(nn.Module):\n",
        "    ''' Embedding layer with weights defined by a vocabulary and an embedding size'''\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        \"\"\"\n",
        "        Initializes a TokenEmbedding module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            size of the vocab that embeddings will be from\n",
        "        emb_size : int\n",
        "            embedding size\n",
        "        \"\"\"\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Returns the embedding of the tokens multiplied by sqrt(emb_size)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokens : torch.Tensor\n",
        "            input Tensor in the form of batched tokenized sentences using\n",
        "            vocabulary indices\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the embeddings\n",
        "        \"\"\"\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    ''' Positional encoding layer for the transformer. '''\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        \"\"\"\n",
        "        Initializes a PositionalEncoding module. The math here isn't important\n",
        "        to have a firm grasp of, but the transformer explanation\n",
        "        linked above does explain how this works visually. :)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emb_size : int\n",
        "            embedding size from TokenEmbedding\n",
        "        dropout : float\n",
        "            dropout to be used after encoding all the inputs\n",
        "        maxlen : int\n",
        "            maximum length of the sentence\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Union[int, torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Returns the sum of the token embedding and the positional embedding.\n",
        "        Again, see the transformer visualization for more information.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token_embedding : torch.Tensor\n",
        "            input Tensor from a TokenEmbedding layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            a positionally-encoded sentence representation\n",
        "        \"\"\"\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZrGd5YvA7ZM0"
      },
      "outputs": [],
      "source": [
        "class ManualTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_ff: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes a ManualTransformer module.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_encoder_layers : int\n",
        "            number of encoder layers in the transformer\n",
        "        num_decoder_layers : int\n",
        "            number of decoder layers in the transformer\n",
        "        emb_size : int\n",
        "            embedding size used for token embedding, positional encoding,\n",
        "            and encoding and decoding\n",
        "        nhead : int\n",
        "            number of heads to use for multi-head attention in the encoder and\n",
        "            decoder\n",
        "        src_vocab_size : int\n",
        "            vocab size of the input sentences of the transformer\n",
        "        tgt_vocab_size : int\n",
        "            vocab size of the target sentences of the transformer\n",
        "        dim_ff : int\n",
        "            hidden dimension of the feedforward layers\n",
        "        dropout : float\n",
        "            dropout to be used after encoding all the inputs\n",
        "        \"\"\"\n",
        "        super(ManualTransformer, self).__init__()\n",
        "\n",
        "        # TODO: Initialize the necessary pieces of the transformer\n",
        "        # Token embeddings and positional encodings\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "\n",
        "        # Encoder and decoder\n",
        "        encoder_layer = ManualEncoderLayer(size=emb_size, dropout=dropout, nhead=nhead, dim_ff=dim_ff)\n",
        "        self.encoder = ManualEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        decoder_layer = ManualDecoderLayer(size=emb_size, dropout=dropout, nhead=nhead, dim_ff=dim_ff)\n",
        "        self.decoder = ManualDecoder(decoder_layer, num_decoder_layers)\n",
        "\n",
        "        # Final projection layer\n",
        "        self.out = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "        ### DO NOT EDIT ###\n",
        "        # Initialize parameters with Glorot / fan_avg.\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def get_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns a mask of the target sequence to be used with attention\n",
        "        during decoding\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tgt : torch.Tensor\n",
        "            model's target sequence\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            boolean mask Tensor displaying locations of tokens in target to\n",
        "            mask attention to\n",
        "        \"\"\"\n",
        "        # TODO: Implement next word mask for the target, which masks out all subsequent words\n",
        "        # HINT: Take a look at torch.triu!\n",
        "        tgt_len = tgt.size(0)  # Sequence length\n",
        "        mask = torch.triu(torch.ones(tgt_len, tgt_len, device=tgt.device), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def encode(self, src: torch.Tensor, src_padding_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full encoding pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        src : torch.Tensor\n",
        "            model's input sequence (in form of input vocab indices)\n",
        "        src_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input sequence\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the full encoder pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the encode function\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        # print(\"Encoder input shape:\", src_emb.shape)\n",
        "        return self.encoder(src_emb, src_padding_mask)\n",
        "\n",
        "    def decode(self,\n",
        "               tgt: torch.Tensor,\n",
        "               src_encoding:torch.Tensor,\n",
        "               src_padding_mask: torch.Tensor,\n",
        "               tgt_padding_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full decoding pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tgt : torch.Tensor\n",
        "            model's target sequence\n",
        "        src_encoding : torch.Tensor\n",
        "            output of the encoder\n",
        "        src_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the \"encoded input\" (src_encoding)\n",
        "        tgt_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input (target sequence)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # TODO: Implement the decode function\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        tgt_mask = self.get_tgt_mask(tgt)\n",
        "        return self.decoder(tgt_emb, src_encoding, src_padding_mask, tgt_mask, tgt_padding_mask)\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,\n",
        "                tgt: torch.Tensor,\n",
        "                src_padding_mask: torch.Tensor,\n",
        "                tgt_padding_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Full forward pass of the transformer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        src : torch.Tensor\n",
        "            model's input sequence (in form of input vocab indices)\n",
        "        tgt : torch.Tensor\n",
        "            model's target sequence (in form of target vocab indices)\n",
        "        src_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the input sequence\n",
        "        tgt_padding_mask : torch.Tensor\n",
        "            boolean mask Tensor displaying locations of padding tokens in\n",
        "            the target sequence\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            output of the forward pass\n",
        "        \"\"\"\n",
        "        # print(\"Entering transformer forward pass\")\n",
        "        # TODO: Implement the forward pass\n",
        "        src_encoding = self.encode(src, src_padding_mask)\n",
        "        # print(\"Encoder output shape:\", src_encoding.shape)\n",
        "        # Decode target sequence\n",
        "        output = self.decode(tgt, src_encoding, src_padding_mask, tgt_padding_mask)\n",
        "        # print(\"Decoder output shape:\", output.shape)\n",
        "        # Project to vocabulary\n",
        "        logits = self.out(output)\n",
        "        # print(\"Final logits shape:\", logits.shape)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOfU5d7t7ZM0"
      },
      "source": [
        "### Training the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zGBPTMc-kf23"
      },
      "outputs": [],
      "source": [
        "def padding_mask(idx_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a boolean mask where True indicates that the token is a padding token.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx_tensor : torch.Tensor\n",
        "        tensor for which a padding mask should be generated\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    torch.Tensor\n",
        "        boolean mask of padding of input tensor\n",
        "    \"\"\"\n",
        "    return (idx_tensor == PAD_IDX).transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IlJd-5dQ7ZM0"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module,\n",
        "                train_dataloader: DataLoader,\n",
        "                loss_fn: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer) -> float:\n",
        "    \"\"\"\n",
        "    Trains the inputted model using the provided data, optimizer, and loss\n",
        "    function for one epoch. Returns the average loss of the epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    train_dataloader : DataLoader\n",
        "        training data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use with training\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    float\n",
        "        epoch average loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_padding_mask = padding_mask(src)\n",
        "        tgt_padding_mask = padding_mask(tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def val(model: nn.Module, val_dataloader: DataLoader, loss_fn: nn.Module) -> float:\n",
        "    \"\"\"\n",
        "    Performs validation on the inputted model using the provided data and loss\n",
        "    function for one epoch. Returns the validation loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    val_dataloader : DataLoader\n",
        "        validation data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use to calculate validation loss\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    float\n",
        "        average validation loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    for src, tgt in val_dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_padding_mask = padding_mask(src)\n",
        "        tgt_padding_mask = padding_mask(tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "88pdxdz97ZM0"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module,\n",
        "          train_dataloader : DataLoader,\n",
        "          loss_fn: nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          n_epochs: int = 10):\n",
        "    \"\"\"\n",
        "    Trains the inputted model using the provided data, optimizer, and loss\n",
        "    function for n epochs. Prints the average loss of the training epoch\n",
        "    and the validation loss after each epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    train_dataloader : DataLoader\n",
        "        training data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use with training\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    n_epochs : int\n",
        "        number of epochs to train for\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        print(f\"Epoch {epoch},\", end = \" \")\n",
        "        start_time = timer()\n",
        "        train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer)\n",
        "        end_time = timer()\n",
        "        print(f\" Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\", end = \" \")\n",
        "\n",
        "        val_loss = val(model, val_loader, loss_fn)\n",
        "        print(f\" Val loss: {val_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOy8a8SD7ZM1",
        "outputId": "6d14d284-63e2-43cb-aabd-c6167bd2e52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1,  Train loss: 4.005, Epoch time = 44.758s  Val loss: 3.058\n",
            "Epoch 2,  Train loss: 2.804, Epoch time = 44.570s  Val loss: 2.473\n",
            "Epoch 3,  Train loss: 2.336, Epoch time = 44.842s  Val loss: 2.186\n",
            "Epoch 4,  Train loss: 2.032, Epoch time = 45.163s  Val loss: 1.986\n",
            "Epoch 5,  Train loss: 1.815, Epoch time = 44.741s  Val loss: 1.895\n",
            "Epoch 6,  Train loss: 1.643, Epoch time = 44.273s  Val loss: 1.811\n",
            "Epoch 7,  Train loss: 1.505, Epoch time = 44.571s  Val loss: 1.773\n",
            "Epoch 8,  Train loss: 1.390, Epoch time = 42.865s  Val loss: 1.740\n",
            "Epoch 9,  Train loss: 1.290, Epoch time = 42.721s  Val loss: 1.727\n",
            "Epoch 10,  Train loss: 1.202, Epoch time = 43.453s  Val loss: 1.715\n",
            "Epoch 11,  Train loss: 1.123, Epoch time = 42.806s  Val loss: 1.706\n",
            "Epoch 12,  Train loss: 1.051, Epoch time = 44.083s  Val loss: 1.706\n",
            "Epoch 13,  Train loss: 0.986, Epoch time = 46.650s  Val loss: 1.724\n",
            "Epoch 14,  Train loss: 0.926, Epoch time = 47.321s  Val loss: 1.744\n",
            "Epoch 15,  Train loss: 0.872, Epoch time = 44.844s  Val loss: 1.766\n"
          ]
        }
      ],
      "source": [
        "# initialize the transformer model\n",
        "manual_transformer = ManualTransformer(num_encoder_layers=3, num_decoder_layers=3, emb_size=512, nhead=8, src_vocab_size=len(de_vocab), tgt_vocab_size=len(en_vocab))\n",
        "\n",
        "# define our loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(manual_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# train the model, change n_epochs to 15 when you're ready!\n",
        "train(manual_transformer, train_loader, loss_fn, optimizer, n_epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ysgiH8OPZb"
      },
      "source": [
        "Congrats! You've finished Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZXfiPs7ZM1"
      },
      "source": [
        "# Part 2: Evaluating our Transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLsYKgbu7ZM1"
      },
      "source": [
        "## Getting Translations Out of Our Model\n",
        "Before we can actually calculate BLEU though, we need to get our outputs from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiXYYMEN7ZM1"
      },
      "source": [
        "### Greedy Decode vs. Beam Search \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "12aKXWs67ZM1"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import softmax\n",
        "import heapq\n",
        "\n",
        "def greedy_decode(model: nn.Module,\n",
        "                  src: torch.Tensor,\n",
        "                  max_len: int,\n",
        "                  start_symbol: int\n",
        "                  ) -> tuple[torch.Tensor, float]:\n",
        "    \"\"\"\n",
        "    Function to generate a translation using a greedy decoding algorithm\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        trained model to decode from\n",
        "    src : torch.Tensor\n",
        "        sequence inputted into model\n",
        "    max_len : int\n",
        "        maximum length to generate with decoding\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    start_symbol : int\n",
        "        vocabulary index to the start of sentence token\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[torch.Tensor, float]\n",
        "        a tuple of a tensor containing the sentence found using greedy\n",
        "        decode represented using vocabulary indices, and the probability\n",
        "        associated with that sentence\n",
        "    \"\"\"\n",
        "\n",
        "    src = src.to(device)\n",
        "    src_padding_mask = padding_mask(src)\n",
        "\n",
        "    total_prob = 1.0\n",
        "\n",
        "    src_encoding = model.encode(src, src_padding_mask).to(device)\n",
        "    seq = torch.ones((1, 1), dtype=torch.long, device=device).fill_(start_symbol)\n",
        "    for i in range(max_len-1):\n",
        "        tgt_padding_mask = padding_mask(seq)\n",
        "        out = model.decode(seq, src_encoding, src_padding_mask, tgt_padding_mask).to(device)\n",
        "        out = out.transpose(0, 1).to(device)\n",
        "        probs = softmax(model.out(out[:, -1]), dim=-1)\n",
        "        prob, next_word = torch.max(probs, dim=1)\n",
        "        total_prob *= prob.item()\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        seq = torch.cat([seq, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return seq.flatten(), total_prob\n",
        "\n",
        "def beam_search(model: nn.Module,\n",
        "                src: torch.Tensor,\n",
        "                max_len: int,\n",
        "                start_symbol: int,\n",
        "                beam_size: int\n",
        "                ) -> tuple[torch.Tensor, float]:\n",
        "    \"\"\"\n",
        "    Function to generate a translation using a beam search algorithm\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        trained model to decode from\n",
        "    src : torch.Tensor\n",
        "        sequence inputted into model\n",
        "    max_len : int\n",
        "        maximum length to generate with decoding\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    start_symbol : int\n",
        "        vocabulary index to the start of sentence token\n",
        "    beam_size : int\n",
        "        size of the beam using during the search\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[torch.Tensor, float]\n",
        "        a tuple of a tensor containing the sentence found using beam\n",
        "        search represented using vocabulary indices, and the probability\n",
        "        associated with that sentence\n",
        "    \"\"\"\n",
        "    # TODO: Implement beam search\n",
        "    src = src.to(device)\n",
        "    src_padding_mask = padding_mask(src)\n",
        "\n",
        "    src_encoding = model.encode(src, src_padding_mask).to(device)\n",
        "\n",
        "    # Priority queue for beam candidates (negative log prob to use heapq as min-heap)\n",
        "    beam = [([start_symbol], 0.0)]\n",
        "    completed_sequences = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beam = []\n",
        "\n",
        "        for seq, score in beam:\n",
        "            if seq[-1] == EOS_IDX:\n",
        "                # If EOS is reached, add the sequence to completed sequences\n",
        "                completed_sequences.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Prepare the input sequence for decoding\n",
        "            seq_tensor = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(1)\n",
        "            tgt_padding_mask = padding_mask(seq_tensor)\n",
        "\n",
        "            # Get model predictions\n",
        "            out = model.decode(seq_tensor, src_encoding, src_padding_mask, tgt_padding_mask).to(device)\n",
        "            out = out.transpose(0, 1).to(device)\n",
        "            probs = softmax(model.out(out[:, -1]), dim=-1)\n",
        "\n",
        "            # Get top beam_size predictions\n",
        "            top_probs, top_indices = torch.topk(probs, beam_size)\n",
        "\n",
        "            # Add new hypotheses to the new beam\n",
        "            for i in range(beam_size):\n",
        "                word = top_indices[0, i].item()\n",
        "                prob = top_probs[0, i].item()\n",
        "                new_seq = seq + [word]\n",
        "                new_score = score - torch.log(torch.tensor(prob)).item()  # Add negative log prob\n",
        "                new_beam.append((new_seq, new_score))\n",
        "\n",
        "        # Keep the top beam_size sequences\n",
        "        beam = heapq.nsmallest(beam_size, new_beam, key=lambda x: x[1])  # Min-heap by score\n",
        "\n",
        "        # If all sequences are completed, break early\n",
        "        if len(completed_sequences) >= beam_size:\n",
        "            break\n",
        "\n",
        "    # Choose the best completed sequence\n",
        "    if completed_sequences:\n",
        "        best_seq, best_score = min(completed_sequences, key=lambda x: x[1])\n",
        "    else:\n",
        "        best_seq, best_score = min(beam, key=lambda x: x[1])\n",
        "\n",
        "    return torch.tensor(best_seq, dtype=torch.long, device=device), torch.exp(torch.tensor(-best_score))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Z0kWZaE1pXny"
      },
      "outputs": [],
      "source": [
        "def tokenlist_to_strlist(tokens: torch.Tensor, vocab: torchtext.vocab.Vocab) -> list[str]:\n",
        "    \"\"\"\n",
        "    Converts a tensor representation of a list of tokens to a list of strings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens : torch.Tensor\n",
        "        tensor representation of a list of tokens\n",
        "    vocab : torchtext.vocab.Vocab\n",
        "        vocabulary associated with the given tokens\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    list[str]\n",
        "        returns a representation of the tokens in the form of a list of word strings\n",
        "    \"\"\"\n",
        "    return vocab.lookup_tokens(list(tokens.cpu().numpy()))\n",
        "\n",
        "def tokenlist_to_str(tokens: torch.Tensor,\n",
        "                     vocab: torchtext.vocab.Vocab,\n",
        "                     show_pad: bool = False\n",
        "                     ) -> str:\n",
        "    \"\"\"\n",
        "    Converts a tensor representation of a list of tokens to a string, ignoring start\n",
        "    and end of sentence tokens, and optionally ignoring padding tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens : torch.Tensor\n",
        "        tensor representation of a list of tokens\n",
        "    vocab : torchtext.vocab.Vocab\n",
        "        vocabulary associated with the given tokens\n",
        "    show_pad : bool\n",
        "        whether the outputted string should contain pad tokens or not\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    str\n",
        "        returns a representation of the tokens in the form of a string\n",
        "    \"\"\"\n",
        "    out = \" \".join(tokenlist_to_strlist(tokens, vocab)).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    if show_pad:\n",
        "        return out\n",
        "    else:\n",
        "        return out.replace(\"<pad>\", \"\")\n",
        "\n",
        "def translate_tokens(model: nn.Module, src_tokens: torch.Tensor) -> tuple[torch.Tensor, float]:\n",
        "    \"\"\"\n",
        "    Takes a tensor of source tokens and \"translates\" them by inputting them into\n",
        "    the model and using either greedy decode or beam search to decode the output\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        trained model to input tokens into\n",
        "    src_tokens : torch.Tensor\n",
        "        tensor representations of input sentences without start and end of\n",
        "        sentence indices\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[torch.Tensor, float]\n",
        "        a tuple of a tensor containing the sentence found using either greedy\n",
        "        decode or beam search represented using vocabulary indices, and the\n",
        "        probability associated with that sentence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src_tokens = torch.cat([torch.tensor([BOS_IDX]), src_tokens, torch.tensor([EOS_IDX])], dim=0).view(-1, 1)\n",
        "    num_tokens = src_tokens.shape[0]\n",
        "\n",
        "    # TODO: uncomment the below line and comment out the greedy_decode line to run beam_search!\n",
        "    # tgt_tokens, prob = beam_search(model, src_tokens, num_tokens + 5, BOS_IDX, 5)\n",
        "    tgt_tokens, prob = greedy_decode(model, src_tokens, num_tokens + 5, BOS_IDX)\n",
        "    return tgt_tokens, prob\n",
        "\n",
        "def translate(model: nn.Module,\n",
        "              src_sentence: str,\n",
        "              output_tokens: bool=False\n",
        "              ) -> tuple[str, torch.Tensor, torch.Tensor] | tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Takes a sentence string and \"translates\" it\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        trained model to input tokens into\n",
        "    src_sentence : str\n",
        "        string representation of input sentence\n",
        "    output_tokens : bool\n",
        "        whether the source and target tokens are also outputted\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[str, torch.Tensor, torch.Tensor]\n",
        "        a string representation of the translated sentence, the tensor\n",
        "        representation of the input, and the tensor representation of the\n",
        "        translated sentence. Used in analyze_sentence below\n",
        "    OR\n",
        "    tuple[str, float]\n",
        "        a string represention of the translated sentence and the\n",
        "        probability associated with that sentence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src_tokens = torch.tensor(de_vocab(de_tokenizer(src_sentence)), dtype=torch.long)\n",
        "    tgt_tokens, prob = translate_tokens(model, src_tokens)\n",
        "    translation = tokenlist_to_str(tgt_tokens, en_vocab)\n",
        "    if output_tokens:\n",
        "      return translation, src_tokens, tgt_tokens\n",
        "    else:\n",
        "      return translation, prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ptnwy3l7ZM1"
      },
      "source": [
        "### Take a look at the outputs!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOW_m8j77ZM2",
        "outputId": "40cd33be-2f4a-466d-e01c-f36c2df42ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A man on a ice field with a dog . \n",
            "  Probability: 0.0009770050634459462\n",
            "\n",
            " Three children eating ice as a parent watches . \n",
            "  Probability: 0.05722860927290408\n",
            "\n",
            " A woman on the beach is taking pictures of the ocean . \n",
            " Probability: 0.04068607463024233\n",
            "\n",
            " A tractor moving dirt for the wall for construction \n",
            "  Probability: 0.0002418453983292781\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def pretty_print(translation: str, prob: float):\n",
        "    \"\"\"\n",
        "    Prints a translation and its associated probability nicely\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    translation : str\n",
        "        translation string\n",
        "    prob : float\n",
        "        probability associated with translation\n",
        "    \"\"\"\n",
        "    print(translation, f\"Probability: {prob}\")\n",
        "    print()\n",
        "\n",
        "pretty_print(*translate(manual_transformer, \"Ein Mann auf einem Maisfeld mit einem Hund\"))\n",
        "pretty_print(*translate(manual_transformer, \"Drei Kinder essen Eis, während ein Elternteil zuschaut\"))\n",
        "pretty_print(*translate(manual_transformer, \"Eine Frau am Strand fotografiert das Meer\"))\n",
        "pretty_print(*translate(manual_transformer, \"Ein Traktor bewegt Erde für den Bau einer Stützmauer.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CmxmPG7ZM2"
      },
      "source": [
        "## Evaluating Translations using BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-n87QUg7ZM2"
      },
      "source": [
        "### Implement BLEU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vLcFxAdhA244"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def my_corpus_bleu(list_reference: list, list_candidate: list, n: int=4):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score given a list of references and a list of candidates\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    list_reference : list[list[str]] | list[list[int]]\n",
        "      list of reference sentences each in the form of a list of words or vocab indices\n",
        "    list_candidate : list[list[str]] | list[list[int]]\n",
        "      list of candidate sentences each in the form of a list of words or vocab indices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the BLEU score given the list of references and candidates\n",
        "    \"\"\"\n",
        "    # TODO: Implement the BLEU algorithm\n",
        "    assert len(list_reference) == len(list_candidate), \"Mismatched reference and candidate sizes.\"\n",
        "\n",
        "    precisions = [0] * n\n",
        "    total_n_grams = [0] * n\n",
        "    ref_length = 0\n",
        "    cand_length = 0\n",
        "\n",
        "    for ref, cand in zip(list_reference, list_candidate):\n",
        "        ref_length += len(ref)\n",
        "        cand_length += len(cand)\n",
        "\n",
        "        for i in range(1, n + 1):\n",
        "            cand_ngrams = Counter(tuple(cand[j:j + i]) for j in range(len(cand) - i + 1))\n",
        "            ref_ngrams = Counter(tuple(ref[j:j + i]) for j in range(len(ref) - i + 1))\n",
        "\n",
        "\n",
        "            for ngram, cand_count in cand_ngrams.items():\n",
        "                precisions[i - 1] += min(cand_count, ref_ngrams.get(ngram, 0))\n",
        "            total_n_grams[i - 1] += sum(cand_ngrams.values())\n",
        "\n",
        "\n",
        "    ngram_precisions = [\n",
        "        (precisions[i] / total_n_grams[i]) if total_n_grams[i] > 0 else 0\n",
        "        for i in range(n)\n",
        "    ]\n",
        "\n",
        "    # Compute brevity penalty\n",
        "    if cand_length > ref_length:\n",
        "        brevity_penalty = 1.0\n",
        "    else:\n",
        "        brevity_penalty = math.exp(1 - ref_length / cand_length) if cand_length > 0 else 0\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu = brevity_penalty * math.exp(sum(math.log(p) for p in ngram_precisions if p > 0) / n)\n",
        "\n",
        "    return bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpnPYAglaZx5"
      },
      "source": [
        "### Checking our BLEU implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYdVaQYsW1iF",
        "outputId": "32648ccb-978a-4d39-d873-236a32f568dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student: 0.2722179122549562\n",
            "NLTK: 0.27221791225495623\n",
            "\n",
            "Student: 0.3387551654364099\n",
            "NLTK: 0.33875516543640977\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "toy_references = [\"The NASA Opportunity rover is battling a massive dust storm on Mars .\".split()]\n",
        "toy_hypothesis = [\"A NASA rover is fighting a massive storm on Mars .\".split()]\n",
        "\n",
        "toy_references2 = [\"The quick brown fox jumped over the lazy dog\".split(),\n",
        "                   \"holy crap today is fring friday\".split(),\n",
        "                   \"jesse we have to cook\". split()]\n",
        "toy_hypothesis2 = [\"The fast maroon fox jumped over the tired old dog\".split(),\n",
        "                   \"oh wow today is fring friday\".split(),\n",
        "                   \"jesse we need to cook\".split()]\n",
        "\n",
        "\n",
        "print(\"Student:\", my_corpus_bleu(toy_references, toy_hypothesis, 4))\n",
        "print(\"NLTK:\", corpus_bleu([[lst] for lst in toy_references], toy_hypothesis)) # labels must be a list of list of words for NLTK BLEU\n",
        "print()\n",
        "print(\"Student:\", my_corpus_bleu(toy_references2, toy_hypothesis2, 4))\n",
        "print(\"NLTK:\", corpus_bleu([[lst] for lst in toy_references2], toy_hypothesis2)) # labels must be a list of list of words for NLTK BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FfiiehDVsF0",
        "outputId": "eceefbe2-f2d5-4b7c-cac5-11fac7666ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student BLEU: 0.3902789602093695\n",
            "NLTK BLEU: 0.3902789602093695\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# remember manual_transformer, val_data, and translate_tokens?\n",
        "predictions = [translate_tokens(manual_transformer, t[0])[0].tolist()[1:-1] for t in val_data]\n",
        "student_labels = [t[1].tolist() for t in val_data] # labels is a list of words for student BLEU\n",
        "nltk_labels = [[t[1].tolist()] for t in val_data] # labels must be a list of list of words for NLTK BLEU\n",
        "\n",
        "print(\"Student BLEU:\", my_corpus_bleu(student_labels, predictions))\n",
        "print(\"NLTK BLEU:\", corpus_bleu(nltk_labels, predictions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
